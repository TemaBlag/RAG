{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4658481a-f484-4d44-a742-52504306561a",
      "metadata": {
        "id": "4658481a-f484-4d44-a742-52504306561a"
      },
      "source": [
        "# Rag From Scratch: Indexing\n",
        "\n",
        "\n",
        "## Preface: Chunking\n",
        "\n",
        "We don't explicity cover document chunking / splitting.\n",
        "\n",
        "For an excellent review of document chunking, see this video from Greg Kamradt:\n",
        "\n",
        "https://www.youtube.com/watch?v=8OJC21T2SL4\n",
        "\n",
        "## Enviornment\n",
        "\n",
        "`(1) Packages`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "_MXXKhKuY6mx"
      },
      "id": "_MXXKhKuY6mx",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fa888bd9-5ad3-40b4-aa88-375d46c13046",
      "metadata": {
        "id": "fa888bd9-5ad3-40b4-aa88-375d46c13046"
      },
      "outputs": [],
      "source": [
        "#! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5822dc8-2e24-4ca3-9bd5-0377633c45d8",
      "metadata": {
        "id": "d5822dc8-2e24-4ca3-9bd5-0377633c45d8"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6098b8bf-354d-4eea-ba25-25fe12ba6b6b",
      "metadata": {
        "id": "6098b8bf-354d-4eea-ba25-25fe12ba6b6b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"key\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afbd20f1-af47-409e-bfbf-3a698b310e7e",
      "metadata": {
        "id": "afbd20f1-af47-409e-bfbf-3a698b310e7e"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c574333e-6a0d-4f4e-8897-783cd71bdcc2",
      "metadata": {
        "id": "c574333e-6a0d-4f4e-8897-783cd71bdcc2"
      },
      "source": [
        "## Part 12: Multi-representation Indexing\n",
        "\n",
        "Docs:\n",
        "\n",
        "https://blog.langchain.dev/semi-structured-multi-modal-rag/\n",
        "\n",
        "https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector\n",
        "\n",
        "Paper:\n",
        "\n",
        "https://arxiv.org/abs/2312.06648"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1bf368e7-ebf6-4469-bfa7-62466184afbb",
      "metadata": {
        "id": "1bf368e7-ebf6-4469-bfa7-62466184afbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5daf0d6-5100-4fbe-91e4-c80d2adbc57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "docs = loader.load()\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
        "docs.extend(loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! pip install langchain_huggingface"
      ],
      "metadata": {
        "id": "J1D5Q_F6eyYb"
      },
      "id": "J1D5Q_F6eyYb",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCZDLP1gesq9",
        "outputId": "73ce74a5-d8f1-4eac-e723-910ed6eb40dd",
        "collapsed": true
      },
      "id": "uCZDLP1gesq9",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []"
      ],
      "metadata": {
        "id": "NDBWByyzSrxa"
      },
      "id": "NDBWByyzSrxa",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(docs):\n",
        "    documents.append(\" \".join(doc.page_content.split()))"
      ],
      "metadata": {
        "id": "fffzjdWihp4a"
      },
      "id": "fffzjdWihp4a",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "431c9506-c6c0-463b-af77-9291a63f1d26",
      "metadata": {
        "id": "431c9506-c6c0-463b-af77-9291a63f1d26"
      },
      "outputs": [],
      "source": [
        "import uuid # provides tools for generating UUIDs (Universally Unique Identifiers)\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chain = (\n",
        "    {\"doc\": lambda x: x[:2000]}\n",
        "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "summaries = chain.batch(documents, {\"max_concurrency\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summaries"
      ],
      "metadata": {
        "id": "0aV50p7Fj1Is",
        "outputId": "b1ebcc37-7b51-4c4a-c731-9fdfdbdee89d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0aV50p7Fj1Is",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"The document explains the concept of LLM (large language model) powered autonomous agents, where LLM acts as the core controller of the agent's brain. The system consists of several components: planning, reflection, and memory. Planning involves breaking down complex tasks into smaller subgoals, reflection enables self-criticism and self-reflection for improvement, and memory allows for both short-term and long-term storage of information. The system also includes the ability to use external tools for\",\n",
              " 'The article discusses the importance of high-quality human data for training machine learning models, as most of the labeled data comes from human annotation. It highlights the need for attention to detail and careful execution during data collection, as human raters contribute to data quality at each stage of the process. Task design, selection and training of raters, and collecting and aggregating data are all important steps that affect data quality. ML techniques can also be used to clean, filter, and smartly aggregate']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dc5614c1-121c-4ad5-8609-cc0e4a633ee9",
      "metadata": {
        "id": "dc5614c1-121c-4ad5-8609-cc0e4a633ee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919412ae-1335-4647-95f4-b98b7d7334ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
            "<ipython-input-12-6abc6854d79f>:8: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore = Chroma(collection_name=\"summaries\",  # unique identifier for a set of related documents or embeddings(namespace)\n"
          ]
        }
      ],
      "source": [
        "from langchain.storage import InMemoryByteStore  # in-memory storage layer for the parent documents\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever # retriever that combines multiple sources of data, allowing retrieval from both vector and byte stores\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"summaries\",  # unique identifier for a set of related documents or embeddings(namespace)\n",
        "                     embedding_function=embeddings)\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore, # for child chunks (summaries).\n",
        "    byte_store=store, # for the parent documents.\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs] # list of unique document IDs (doc_ids) is generated for each parent document in the docs\n",
        "\n",
        "# Docs linked to summaries\n",
        "summary_docs = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(summaries)\n",
        "]\n",
        "\n",
        "# Add\n",
        "retriever.vectorstore.add_documents(summary_docs)\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs))) # store multiple key-value pairs in the document store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f111ca83-3e56-4785-bac3-99948cd8df1b",
      "metadata": {
        "id": "f111ca83-3e56-4785-bac3-99948cd8df1b",
        "outputId": "73e56530-971f-4b42-ee2f-c7673567ca3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'doc_id': 'c740431b-556d-470e-96a0-1a79948d726a'}, page_content=\"The document explains the concept of LLM (large language model) powered autonomous agents, where LLM acts as the core controller of the agent's brain. The system consists of several components: planning, reflection, and memory. Planning involves breaking down complex tasks into smaller subgoals, reflection enables self-criticism and self-reflection for improvement, and memory allows for both short-term and long-term storage of information. The system also includes the ability to use external tools for\")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "query = \"Memory in agents\"\n",
        "sub_docs = vectorstore.similarity_search(query,k=1)\n",
        "sub_docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "729074f9-8bde-4c76-a7da-4cc0e50ed52d",
      "metadata": {
        "id": "729074f9-8bde-4c76-a7da-4cc0e50ed52d",
        "outputId": "72f300b3-0ac2-473d-bf75-3837e638dcfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-f30970f6fe47>:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
        "retrieved_docs[0].page_content[0:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a52d8214-a997-4761-a8a3-0a29109410be",
      "metadata": {
        "id": "a52d8214-a997-4761-a8a3-0a29109410be"
      },
      "source": [
        "Related idea is the [parent document retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10e04037-03c6-49c4-8d14-ad35650fc654",
      "metadata": {
        "id": "10e04037-03c6-49c4-8d14-ad35650fc654"
      },
      "source": [
        "## Part 13: RAPTOR\n",
        "\n",
        "Flow:\n",
        "\n",
        "Deep dive video:\n",
        "\n",
        "https://www.youtube.com/watch?v=jbGchdTL7d0\n",
        "\n",
        "Paper:\n",
        "\n",
        "https://arxiv.org/pdf/2401.18059.pdf\n",
        "\n",
        "Full code:\n",
        "\n",
        "https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d09151ce-aea1-4574-84ab-a72f17bf59b4",
      "metadata": {
        "id": "d09151ce-aea1-4574-84ab-a72f17bf59b4"
      },
      "source": [
        "## Part 14: ColBERT\n",
        "\n",
        "RAGatouille makes it as simple to use ColBERT.\n",
        "\n",
        "ColBERT generates a contextually influenced vector for each token in the passages.\n",
        "\n",
        "ColBERT similarly generates vectors for each token in the query.\n",
        "\n",
        "Then, the score of each document is the sum of the maximum similarity of each query embedding to any of the document embeddings:\n",
        "\n",
        "See [here](https://hackernoon.com/how-colbert-helps-developers-overcome-the-limits-of-rag) and [here](https://python.langchain.com/docs/integrations/retrievers/ragatouille) and [here](https://til.simonwillison.net/llms/colbert-ragatouille)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "79676de8-c93c-415a-bd1a-ba64065bae27",
      "metadata": {
        "id": "79676de8-c93c-415a-bd1a-ba64065bae27"
      },
      "outputs": [],
      "source": [
        "#! pip install -U ragatouille # library designed for efficient retrieval-augmented generation tasks, building upon the principles of retrieval-augmented generation architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "96deaaa9-5101-48a5-a93d-b8af0122430f",
      "metadata": {
        "id": "96deaaa9-5101-48a5-a93d-b8af0122430f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1824012f-83d5-49ab-c6ce-a9783be1381d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 16, 17:33:53] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel # handle pretrained models that are suitable for retrieval-augmented generation tasks\n",
        "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "10b9bfc1-5f2b-4b9e-9934-9844e3b60646",
      "metadata": {
        "id": "10b9bfc1-5f2b-4b9e-9934-9844e3b60646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d094f28-68f6-40da-e59b-2041da592423"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67531\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def get_wikipedia_page(title: str):\n",
        "    \"\"\"\n",
        "    Retrieve the full text content of a Wikipedia page.\n",
        "\n",
        "    :param title: str - Title of the Wikipedia page.\n",
        "    :return: str - Full text content of the page as raw string.\n",
        "    \"\"\"\n",
        "    # Wikipedia API endpoint\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    # Parameters for the API request\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": title,\n",
        "        \"prop\": \"extracts\", # indicates that you want to extract the plain text content of the page.\n",
        "        \"explaintext\": True,\n",
        "    }\n",
        "\n",
        "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
        "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
        "\n",
        "    response = requests.get(URL, params=params, headers=headers)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extracting page content\n",
        "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
        "    return page[\"extract\"] if \"extract\" in page else None\n",
        "\n",
        "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
        "print(len(full_document))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_document[:300]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "9Sd0N3-wmatY",
        "outputId": "173d7faf-08ef-48b5-b99c-7c0318637737"
      },
      "id": "9Sd0N3-wmatY",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hayao Miyazaki (宮崎 駿 or 宮﨑 駿, Miyazaki Hayao, [mijaꜜzaki hajao]; born January 5, 1941) is a Japanese animator, filmmaker, and manga artist. A founder of Studio Ghibli, he has attained international acclaim as a masterful storyteller and creator of Japanese animated feature films, and is widely regar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a2317cc1-7406-4115-84c2-d0527a4ad22e",
      "metadata": {
        "id": "a2317cc1-7406-4115-84c2-d0527a4ad22e",
        "outputId": "d26fda41-a442-418a-a2ea-eb23347d4227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- WARNING! You are using PLAID with an experimental replacement for FAISS for greater compatibility ----\n",
            "This is a behaviour change from RAGatouille 0.8.0 onwards.\n",
            "This works fine for most users and smallish datasets, but can be considerably slower than FAISS and could cause worse results in some situations.\n",
            "If you're confident with FAISS working on your machine, pass use_faiss=True to revert to the FAISS-using behaviour.\n",
            "--------------------\n",
            "\n",
            "\n",
            "[Oct 16, 17:34:39] #> Creating directory .ragatouille/colbert/indexes/Miyazaki-123 \n",
            "\n",
            "\n",
            "[Oct 16, 17:34:40] [0] \t\t #> Encoding 121 passages..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:22<00:00, 20.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 16, 17:36:03] [0] \t\t avg_doclen_est = 131.39669799804688 \t len(local_sample) = 121\n",
            "[Oct 16, 17:36:03] [0] \t\t Creating 1,024 partitions.\n",
            "[Oct 16, 17:36:03] [0] \t\t *Estimated* 15,899 embeddings.\n",
            "[Oct 16, 17:36:03] [0] \t\t #> Saving the indexing plan to .ragatouille/colbert/indexes/Miyazaki-123/plan.json ..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "used 20 iterations (7.8104s) to cluster 15105 items into 1024 clusters\n",
            "[0.036, 0.042, 0.04, 0.037, 0.033, 0.039, 0.034, 0.037, 0.036, 0.034, 0.036, 0.039, 0.036, 0.038, 0.036, 0.04, 0.034, 0.035, 0.035, 0.04, 0.036, 0.037, 0.037, 0.036, 0.039, 0.032, 0.041, 0.036, 0.037, 0.037, 0.038, 0.041, 0.039, 0.036, 0.036, 0.034, 0.038, 0.036, 0.037, 0.04, 0.036, 0.04, 0.034, 0.037, 0.037, 0.034, 0.036, 0.041, 0.038, 0.034, 0.036, 0.036, 0.036, 0.038, 0.038, 0.037, 0.039, 0.04, 0.043, 0.033, 0.036, 0.038, 0.036, 0.036, 0.038, 0.036, 0.039, 0.039, 0.033, 0.033, 0.037, 0.036, 0.035, 0.038, 0.036, 0.034, 0.036, 0.039, 0.035, 0.036, 0.039, 0.036, 0.033, 0.041, 0.034, 0.034, 0.039, 0.036, 0.034, 0.042, 0.037, 0.037, 0.035, 0.038, 0.036, 0.035, 0.04, 0.035, 0.039, 0.038, 0.04, 0.042, 0.039, 0.036, 0.039, 0.038, 0.039, 0.036, 0.037, 0.033, 0.038, 0.036, 0.036, 0.033, 0.036, 0.039, 0.038, 0.037, 0.037, 0.038, 0.034, 0.034, 0.034, 0.039, 0.036, 0.038, 0.04, 0.037]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 16, 17:36:11] [0] \t\t #> Encoding 121 passages..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 25%|██▌       | 1/4 [00:25<01:15, 25.24s/it]\u001b[A\n",
            " 50%|█████     | 2/4 [00:44<00:43, 21.81s/it]\u001b[A\n",
            " 75%|███████▌  | 3/4 [01:04<00:20, 20.77s/it]\u001b[A\n",
            "100%|██████████| 4/4 [01:18<00:00, 19.75s/it]\n",
            "1it [01:19, 79.50s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00, 471.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 16, 17:37:30] #> Optimizing IVF to store map from centroids to list of pids..\n",
            "[Oct 16, 17:37:30] #> Building the emb2pid mapping..\n",
            "[Oct 16, 17:37:30] len(emb2pid) = 15899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1024/1024 [00:00<00:00, 45755.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 16, 17:37:30] #> Saved optimized IVF to .ragatouille/colbert/indexes/Miyazaki-123/ivf.pid.pt\n",
            "Done indexing!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.ragatouille/colbert/indexes/Miyazaki-123'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "RAG.index(\n",
        "    collection=[full_document],\n",
        "    index_name=\"Miyazaki-123\",\n",
        "    max_document_length=180,\n",
        "    split_documents=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f929e4fd-2175-465d-bd88-664f67caa576",
      "metadata": {
        "id": "f929e4fd-2175-465d-bd88-664f67caa576",
        "outputId": "cec0ed9b-44ae-4e43-e875-26107afc5932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading searcher for index Miyazaki-123 for the first time... This may take a few seconds\n",
            "[Oct 16, 17:37:31] #> Loading codec...\n",
            "[Oct 16, 17:37:31] #> Loading IVF...\n",
            "[Oct 16, 17:37:31] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "[Oct 16, 17:38:19] #> Loading doclens...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2044.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 16, 17:38:19] #> Loading codes and residuals...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 176.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 16, 17:38:19] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Oct 16, 17:38:59] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
            "Searcher loaded!\n",
            "\n",
            "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
            "#> Input: . What animation studio did Miyazaki found?, \t\t True, \t\t None\n",
            "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  7284,  2996,  2106,  2771,  3148, 18637,  2179,\n",
            "         1029,   102,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
            "          103,   103])\n",
            "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'content': '=== Studio Ghibli ===\\n\\n\\n==== Early films (1985–1995) ====\\nFollowing the success of Nausicaä of the Valley of the Wind, Miyazaki and Takahata founded the animation production company Studio Ghibli on June 15, 1985, as a subsidiary of Tokuma Shoten, with offices in Kichijōji designed by Miyazaki. Miyazaki named the studio after the Caproni Ca.309 and the Italian word meaning \"a hot wind that blows in the desert\"; the name had been registered a year earlier.',\n",
              "  'score': 25.885807037353516,\n",
              "  'rank': 1,\n",
              "  'document_id': '7165b56d-60d6-41c0-b684-876203864c77',\n",
              "  'passage_id': 42},\n",
              " {'content': 'Hayao Miyazaki (宮崎 駿 or 宮﨑 駿, Miyazaki Hayao, [mijaꜜzaki hajao]; born January 5, 1941) is a Japanese animator, filmmaker, and manga artist. A founder of Studio Ghibli, he has attained international acclaim as a masterful storyteller and creator of Japanese animated feature films, and is widely regarded as one of the most accomplished filmmakers in the history of animation.\\nBorn in Tokyo City, Miyazaki expressed interest in manga and animation from an early age.',\n",
              "  'score': 25.51839828491211,\n",
              "  'rank': 2,\n",
              "  'document_id': '7165b56d-60d6-41c0-b684-876203864c77',\n",
              "  'passage_id': 0},\n",
              " {'content': \"Miyazaki, initially reluctant, countered that an hour-long animation would be more suitable, and Tokuma Shoten agreed on a feature-length film.\\nProduction began on May 31, 1983, with animation beginning in August; funding was provided through a joint venture between Tokuma Shoten and the advertising agency Hakuhodo, for whom Miyazaki's youngest brother worked. Animation studio Topcraft was chosen as the production house. Miyazaki found some of Topcraft's staff unreliable, and brought on several of his previous collaborators, including Takahata, who served as producer, though he was reluctant to do so. Pre-production began on May 31, 1983; Miyazaki encountered difficulties in creating the screenplay, with only sixteen chapters of the manga to work with.\",\n",
              "  'score': 25.32529640197754,\n",
              "  'rank': 3,\n",
              "  'document_id': '7165b56d-60d6-41c0-b684-876203864c77',\n",
              "  'passage_id': 38}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "results = RAG.search(query=\"What animation studio did Miyazaki found?\", k=3)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ca1cbbc7-bd6e-488d-9419-740a62eb097a",
      "metadata": {
        "id": "ca1cbbc7-bd6e-488d-9419-740a62eb097a",
        "outputId": "35c74ae3-ec21-4680-f1c5-c50b3dfadb0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={}, page_content='=== Studio Ghibli ===\\n\\n\\n==== Early films (1985–1995) ====\\nFollowing the success of Nausicaä of the Valley of the Wind, Miyazaki and Takahata founded the animation production company Studio Ghibli on June 15, 1985, as a subsidiary of Tokuma Shoten, with offices in Kichijōji designed by Miyazaki. Miyazaki named the studio after the Caproni Ca.309 and the Italian word meaning \"a hot wind that blows in the desert\"; the name had been registered a year earlier.'),\n",
              " Document(metadata={}, page_content='Hayao Miyazaki (宮崎 駿 or 宮﨑 駿, Miyazaki Hayao, [mijaꜜzaki hajao]; born January 5, 1941) is a Japanese animator, filmmaker, and manga artist. A founder of Studio Ghibli, he has attained international acclaim as a masterful storyteller and creator of Japanese animated feature films, and is widely regarded as one of the most accomplished filmmakers in the history of animation.\\nBorn in Tokyo City, Miyazaki expressed interest in manga and animation from an early age.'),\n",
              " Document(metadata={}, page_content=\"Miyazaki, initially reluctant, countered that an hour-long animation would be more suitable, and Tokuma Shoten agreed on a feature-length film.\\nProduction began on May 31, 1983, with animation beginning in August; funding was provided through a joint venture between Tokuma Shoten and the advertising agency Hakuhodo, for whom Miyazaki's youngest brother worked. Animation studio Topcraft was chosen as the production house. Miyazaki found some of Topcraft's staff unreliable, and brought on several of his previous collaborators, including Takahata, who served as producer, though he was reluctant to do so. Pre-production began on May 31, 1983; Miyazaki encountered difficulties in creating the screenplay, with only sixteen chapters of the manga to work with.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "retriever = RAG.as_langchain_retriever(k=3)\n",
        "retriever.invoke(\"What animation studio did Miyazaki found?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "promt = \"\"\"You are a helpful assistant. Your task will be to give a response using context:\n",
        "{context}\n",
        "Give answer for this query: {query}\n",
        "\"\"\"\n",
        "\n",
        "def create_context(docs:list):\n",
        "    context = \"\"\n",
        "    for doc in docs:\n",
        "        context += doc.page_content\n",
        "    return context\n",
        "\n",
        "\n",
        "promt = ChatPromptTemplate.from_template(promt)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        'context': lambda inputs: create_context(retriever.invoke(inputs['question'])),\n",
        "        'query': itemgetter('question')\n",
        "    }\n",
        "    | promt\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "OlzDx0Grsmm9"
      },
      "id": "OlzDx0Grsmm9",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({'question': \"What animation studio did Miyazaki found?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "LhZfpD0ytFnM",
        "outputId": "89479555-b888-4444-9e8b-a9712a31fe1f"
      },
      "id": "LhZfpD0ytFnM",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n",
            "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Forbidden\"}')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Miyazaki founded the animation production company Studio Ghibli on June 15, 1985.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}